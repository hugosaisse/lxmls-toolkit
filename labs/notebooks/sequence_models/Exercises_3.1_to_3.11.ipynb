{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other imports.\n",
    "from lxmls import DATA_PATH\n",
    "import lxmls.readers.simple_sequence as ssr\n",
    "import lxmls.sequences.hmm as hmmc\n",
    "import lxmls.readers.pos_corpus as pcc\n",
    "import lxmls.sequences.confusion_matrix as cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.1\n",
    "Load the simple sequence dataset. From the ipython command line create a simple sequence object and look at the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[walk/rainy walk/sunny shop/sunny clean/sunny , walk/rainy walk/rainy shop/rainy clean/sunny , walk/sunny shop/sunny shop/sunny clean/sunny ]\n",
      "[walk/rainy walk/sunny shop/sunny clean/sunny , clean/sunny walk/sunny tennis/sunny walk/sunny ]\n"
     ]
    }
   ],
   "source": [
    "simple = ssr.SimpleSequence()\n",
    "print(simple.train)\n",
    "print(simple.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walk/rainy walk/sunny shop/sunny clean/sunny \n",
      "walk/rainy walk/rainy shop/rainy clean/sunny \n",
      "walk/sunny shop/sunny shop/sunny clean/sunny \n"
     ]
    }
   ],
   "source": [
    "for sequence in simple.train.seq_list:\n",
    "    print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 2]\n",
      "[0, 0, 1, 2]\n",
      "[0, 1, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "for sequence in simple.train.seq_list:\n",
    "    print(sequence.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 1]\n",
      "[0, 0, 0, 1]\n",
      "[1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "for sequence in simple.train.seq_list:\n",
    "    print(sequence.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[walk/rainy walk/sunny shop/sunny clean/sunny , walk/rainy walk/rainy shop/rainy clean/sunny , walk/sunny shop/sunny shop/sunny clean/sunny ]\n"
     ]
    }
   ],
   "source": [
    "print(simple.train.seq_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.2\n",
    "The provided function train_supervised from the hmm.py file implements the above parameter estimates.\n",
    "Run this function given the simple dataset above and look at the estimated probabilities. Are they correct? You can also\n",
    "check the variables ending in counts instead of probs to see the raw counts (for example, typing hmm.initial counts\n",
    "will show you the raw counts of initial states). How are the counts related to the probabilities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walk/rainy walk/sunny shop/sunny clean/sunny \n",
      "walk/rainy walk/rainy shop/rainy clean/sunny \n",
      "walk/sunny shop/sunny shop/sunny clean/sunny \n"
     ]
    }
   ],
   "source": [
    "for sequence in simple.train.seq_list:\n",
    "    print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Probabilities:\n",
      "[0.66666667 0.33333333]\n",
      "[2. 1.]\n"
     ]
    }
   ],
   "source": [
    "hmm = hmmc.HMM(simple.x_dict, simple.y_dict)\n",
    "hmm.train_supervised(simple.train)\n",
    "\n",
    "print(\"Initial Probabilities:\")\n",
    "print(hmm.initial_probs)\n",
    "print(hmm.initial_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition Probabilities:\n",
      "[[0.5   0.   ]\n",
      " [0.5   0.625]]\n",
      "[[2. 0.]\n",
      " [2. 5.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Transition Probabilities:\")\n",
    "print(hmm.transition_probs)\n",
    "print(hmm.transition_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Probabilities:\n",
      "[0.    0.375]\n",
      "[0. 3.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Probabilities:\")\n",
    "print(hmm.final_probs)\n",
    "print(hmm.final_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emission Probabilities\n",
      "[[0.75  0.25 ]\n",
      " [0.25  0.375]\n",
      " [0.    0.375]\n",
      " [0.    0.   ]]\n",
      "[[3. 2.]\n",
      " [1. 3.]\n",
      " [0. 3.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Emission Probabilities\")\n",
    "print(hmm.emission_probs)\n",
    "print(hmm.emission_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.3 \n",
    "Convince yourself that the score of a path in the trellis (summing over the scores above) is equivalent to the\n",
    "log-probability $\\log P(X = x, Y = y)$, as defined in Eq. 2.2. Use the given function compute_scores on the first training\n",
    "sequence and confirm that the values are correct. You should get the same values as presented below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.40546511 -1.09861229]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hugos\\onedrive\\nova ims doctoral program\\lxmls 2022\\lxmls-toolkit\\lxmls\\sequences\\hmm.py:195: RuntimeWarning: divide by zero encountered in log\n",
      "  transition_scores[pos-1, :, :] = np.log(self.transition_probs)\n",
      "c:\\users\\hugos\\onedrive\\nova ims doctoral program\\lxmls 2022\\lxmls-toolkit\\lxmls\\sequences\\hmm.py:193: RuntimeWarning: divide by zero encountered in log\n",
      "  emission_scores[pos, :] = np.log(self.emission_probs[sequence.x[pos], :])\n",
      "c:\\users\\hugos\\onedrive\\nova ims doctoral program\\lxmls 2022\\lxmls-toolkit\\lxmls\\sequences\\hmm.py:198: RuntimeWarning: divide by zero encountered in log\n",
      "  final_scores = np.log(self.final_probs)\n"
     ]
    }
   ],
   "source": [
    "initial_scores, transition_scores, final_scores, emission_scores = \\\n",
    "    hmm.compute_scores(simple.train.seq_list[0])\n",
    "print(initial_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.69314718        -inf]\n",
      "  [-0.69314718 -0.47000363]]\n",
      "\n",
      " [[-0.69314718        -inf]\n",
      "  [-0.69314718 -0.47000363]]\n",
      "\n",
      " [[-0.69314718        -inf]\n",
      "  [-0.69314718 -0.47000363]]]\n"
     ]
    }
   ],
   "source": [
    "print(transition_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[       -inf -0.98082925]\n"
     ]
    }
   ],
   "source": [
    "print(final_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.28768207 -1.38629436]\n",
      " [-0.28768207 -1.38629436]\n",
      " [-1.38629436 -0.98082925]\n",
      " [       -inf -0.98082925]]\n"
     ]
    }
   ],
   "source": [
    "print(emission_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.4 \n",
    "Look at the module ``sequences/log_domain.py``. This module implements a function ``logsum_pair(logx, logy)`` to add two numbers represented in the log-domain; it returns their sum also represented in the log-domain. The\n",
    "function logsum(logv) sums all components of an array represented in the log-domain. This will be used later in our\n",
    "decoding algorithms. To observe why this is important, type the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8743319960178804\n",
      "9.52373221055498\n",
      "89.0837099570808\n",
      "inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugos\\AppData\\Local\\Temp\\ipykernel_14716\\1739973214.py:7: RuntimeWarning: overflow encountered in exp\n",
      "  print(np.log(sum(np.exp(1000*a))))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.random.rand(10)\n",
    "print(np.log(sum(np.exp(a))))\n",
    "print(np.log(sum(np.exp(10*a))))\n",
    "print(np.log(sum(np.exp(100*a))))\n",
    "print(np.log(sum(np.exp(1000*a))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.874331996017881\n",
      "9.52373221055498\n",
      "89.0837099570808\n",
      "890.825458361828\n"
     ]
    }
   ],
   "source": [
    "from lxmls.sequences.log_domain import logsum\n",
    "\n",
    "print(logsum(a))\n",
    "print(logsum(10*a))\n",
    "print(logsum(100*a))\n",
    "print(logsum(1000*a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.5 \n",
    "Run the provided forward-backward algorithm on the first train sequence. Observe that both the forward\n",
    "and the backward passes give the same log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-Likelihood = -5.068232326005127\n"
     ]
    }
   ],
   "source": [
    "log_likelihood, forward = hmm.decoder.run_forward(initial_scores, transition_scores, final_scores, emission_scores)\n",
    "print('Log-Likelihood = {}'.format(log_likelihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-Likelihood = -5.068232326005126\n"
     ]
    }
   ],
   "source": [
    "log_likelihood, backward = hmm.decoder.run_backward(initial_scores, transition_scores, final_scores, emission_scores)\n",
    "print('Log-Likelihood = {}'.format(log_likelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.6 \n",
    "Compute the node posteriors for the first training sequence (use the provided compute posteriors function), and look at the output. Note that the state posteriors are a proper probability distribution (the lines of the result\n",
    "sum to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.95738152 0.04261848]\n",
      " [0.75281282 0.24718718]\n",
      " [0.26184794 0.73815206]\n",
      " [0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "initial_scores, transition_scores, final_scores, emission_scores = \\\n",
    "    hmm.compute_scores(simple.train.seq_list[0])\n",
    "state_posteriors, _, _ = hmm.compute_posteriors(initial_scores,\n",
    "                                                transition_scores,\n",
    "                                                final_scores,\n",
    "                                                emission_scores)\n",
    "print(state_posteriors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.7 \n",
    "Run the posterior decode on the first test sequence, and evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction test 0: walk/rainy walk/rainy shop/sunny clean/sunny \n",
      "Truth test 0: walk/rainy walk/sunny shop/sunny clean/sunny \n"
     ]
    }
   ],
   "source": [
    "y_pred = hmm.posterior_decode(simple.test.seq_list[0])\n",
    "print(\"Prediction test 0: {}\".format(y_pred))\n",
    "print(\"Truth test 0: {}\".format(simple.test.seq_list[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same for the second test sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction test 1 clean/rainy walk/rainy tennis/rainy walk/rainy \n",
      "Truth test 1 clean/sunny walk/sunny tennis/sunny walk/sunny \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hugos\\onedrive\\nova ims doctoral program\\lxmls 2022\\lxmls-toolkit\\lxmls\\sequences\\sequence_classifier.py:80: RuntimeWarning: invalid value encountered in subtract\n",
      "  state_posteriors[pos, :] -= log_likelihood\n",
      "c:\\users\\hugos\\onedrive\\nova ims doctoral program\\lxmls 2022\\lxmls-toolkit\\lxmls\\sequences\\sequence_classifier.py:93: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  transition_posteriors[pos, state, prev_state] -= log_likelihood\n"
     ]
    }
   ],
   "source": [
    "y_pred = hmm.posterior_decode(simple.test.seq_list[1])\n",
    "print(\"Prediction test 1\", y_pred)\n",
    "print(\"Truth test 1\", simple.test.seq_list[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is wrong? Note the observations for the second test sequence: the observation tennis was never seen at\n",
    "training time, so the probability for it will be zero (no matter what state). This will make all possible state sequences have\n",
    "zero probability. As seen in the previous lecture, this is a problem with generative models, which can be corrected using\n",
    "smoothing (among other options)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the train supervised method to add smoothing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    def train_supervised(self,sequence_list, smoothing):\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "````\n",
    "def train_supervised(self, dataset, smoothing=0):\n",
    "    \"\"\" Train an HMM from a list of sequences containing observations\n",
    "    and the gold states. This is just counting and normalizing.\"\"\"\n",
    "    # Set all counts to zeros (optionally, smooth).\n",
    "    self.clear_counts(smoothing)\n",
    "    # Count occurrences of events.\n",
    "    self.collect_counts_from_corpus(dataset)\n",
    "    # Normalize to get probabilities.\n",
    "    self.compute_parameters()\n",
    "````\n",
    "Mode detaials see class HMM in lxmls.sequences.hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction test 0 with smoothing\n",
      "walk/rainy walk/rainy shop/sunny clean/sunny \n",
      "Truth test 0\n",
      "walk/rainy walk/sunny shop/sunny clean/sunny \n"
     ]
    }
   ],
   "source": [
    "hmm.train_supervised(simple.train, smoothing=0.1)\n",
    "\n",
    "y_pred = hmm.posterior_decode(simple.test.seq_list[0])\n",
    "print(\"Prediction test 0 with smoothing\")\n",
    "print(y_pred)\n",
    "print(\"Truth test 0\")\n",
    "print(simple.test.seq_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction test 1 with smoothing\n",
      "clean/sunny walk/sunny tennis/sunny walk/sunny \n",
      "Truth test 1\n",
      "clean/sunny walk/sunny tennis/sunny walk/sunny \n"
     ]
    }
   ],
   "source": [
    "y_pred = hmm.posterior_decode(simple.test.seq_list[1])\n",
    "print(\"Prediction test 1 with smoothing\")\n",
    "print(y_pred)\n",
    "print(\"Truth test 1\")\n",
    "print(simple.test.seq_list[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.8\n",
    "\n",
    "Implement a method for performing Viterbi decoding in file ``sequence classification decoder.py``.\n",
    "\n",
    "*Hint:* look at the implementation of ``run forward``. Also check the help for the numpy methods max and argmax.\n",
    "This method will be called by\n",
    "\n",
    "``def viterbi_decode(self, sequence)``\n",
    "\n",
    "in the module sequence classifier.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "    def run_viterbi(self, initial_scores, transition_scores, final_scores, emission_scores):\n",
    "\n",
    "        length = np.size(emission_scores, 0) # Length of the sequence.\n",
    "        num_states = np.size(initial_scores) # Number of states.\n",
    "\n",
    "        # Viterbi variables.\n",
    "        viterbi = np.zeros([length, num_states]) + logzero()\n",
    "        backtrack = np.zeros([length, num_states], dtype=int)\n",
    "\n",
    "        # Initialization.\n",
    "        viterbi[0,:] = emission_scores[0,:] + initial_scores\n",
    "\n",
    "        # viterbi loop.\n",
    "        for pos in xrange(1,length):\n",
    "            for current_state in xrange(num_states):\n",
    "                # Note the fact that multiplication in log domain turns a sum and sum turns a logsum\n",
    "                viterbi_score = viterbi[pos-1, :] + transition_scores[pos-1, current_state, :]\n",
    "                viterbi[pos, current_state] = np.max(viterbi_score)\n",
    "                viterbi[pos, current_state] += emission_scores[pos, current_state]\n",
    "                backtrack[pos, current_state] = np.argmax(viterbi_score)\n",
    "\n",
    "        best_score = np.max(viterbi[-1, :] + final_scores)\n",
    "        \n",
    "        best_path = np.zeros(length, dtype=int)\n",
    "        best_path[-1] = np.argmax(viterbi[-1, :] + final_scores)\n",
    "\n",
    "        for pos in xrange(length-2,-1,-1):\n",
    "            #best_path[pos] = int(np.argmax(backtrack[pos+1]))\n",
    "            best_path[pos] = backtrack[pos+1, best_path[pos+1]]\n",
    "\n",
    "        return best_path , best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi decoding Prediction test 0 with smoothing\n",
      "walk/rainy walk/rainy shop/sunny clean/sunny  -6.020501246982869\n",
      "Truth test 0\n",
      "walk/rainy walk/sunny shop/sunny clean/sunny \n"
     ]
    }
   ],
   "source": [
    "y_pred, score = hmm.viterbi_decode(simple.test.seq_list[0])\n",
    "print(\"Viterbi decoding Prediction test 0 with smoothing\")\n",
    "print(y_pred, score)\n",
    "print(\"Truth test 0\")\n",
    "print(simple.test.seq_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi decoding Prediction test 1 with smoothing\n",
      "clean/sunny walk/sunny tennis/sunny walk/sunny  -11.713974073970887\n",
      "Truth test 1\n",
      "clean/sunny walk/sunny tennis/sunny walk/sunny \n"
     ]
    }
   ],
   "source": [
    "y_pred, score = hmm.viterbi_decode(simple.test.seq_list[1])\n",
    "print(\"Viterbi decoding Prediction test 1 with smoothing\")\n",
    "print(y_pred, score)\n",
    "print(\"Truth test 1\")\n",
    "print(simple.test.seq_list[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.9 \n",
    "Test the model using both posterior decoding and Viterbi decoding on both the train and test set, using the\n",
    "methods in class HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "corpus = pcc.PostagCorpus()\n",
    "train_seq = corpus.read_sequence_list_conll(\"%s/train-02-21.conll\" % DATA_PATH, max_sent_len=15, max_nr_sent=1000)\n",
    "test_seq = corpus.read_sequence_list_conll(\"%s/test-23.conll\" % DATA_PATH, max_sent_len=15, max_nr_sent=1000)\n",
    "dev_seq = corpus.read_sequence_list_conll(\"%s/dev-22.conll\" % DATA_PATH, max_sent_len=15, max_nr_sent=1000)\n",
    "hmm = hmmc.HMM(corpus.word_dict, corpus.tag_dict)\n",
    "hmm.train_supervised(train_seq)\n",
    "hmm.print_transition_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hugos\\onedrive\\nova ims doctoral program\\lxmls 2022\\lxmls-toolkit\\lxmls\\sequences\\hmm.py:193: RuntimeWarning: divide by zero encountered in log\n",
      "  emission_scores[pos, :] = np.log(self.emission_probs[sequence.x[pos], :])\n",
      "c:\\users\\hugos\\onedrive\\nova ims doctoral program\\lxmls 2022\\lxmls-toolkit\\lxmls\\sequences\\hmm.py:195: RuntimeWarning: divide by zero encountered in log\n",
      "  transition_scores[pos-1, :, :] = np.log(self.transition_probs)\n",
      "c:\\users\\hugos\\onedrive\\nova ims doctoral program\\lxmls 2022\\lxmls-toolkit\\lxmls\\sequences\\hmm.py:198: RuntimeWarning: divide by zero encountered in log\n",
      "  final_scores = np.log(self.final_probs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Accuracy: Posterior Decode 0.985, Viterbi Decode: 0.985\n"
     ]
    }
   ],
   "source": [
    "viterbi_pred_train = hmm.viterbi_decode_corpus(train_seq)\n",
    "posterior_pred_train = hmm.posterior_decode_corpus(train_seq)\n",
    "eval_viterbi_train = hmm.evaluate_corpus(train_seq, viterbi_pred_train)\n",
    "eval_posterior_train = hmm.evaluate_corpus(train_seq, posterior_pred_train)\n",
    "print(\"Train Set Accuracy: Posterior Decode %.3f, Viterbi Decode: %.3f\"%(\n",
    "eval_posterior_train,eval_viterbi_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viterbi_pred_test = hmm.viterbi_decode_corpus(test_seq)\n",
    "posterior_pred_test = hmm.posterior_decode_corpus(test_seq)\n",
    "eval_viterbi_test = hmm.evaluate_corpus(test_seq, viterbi_pred_test)\n",
    "eval_posterior_test = hmm.evaluate_corpus(test_seq, posterior_pred_test)\n",
    "print(\"Test Set Accuracy: Posterior Decode %.3f, Viterbi Decode: %.3f\" % (eval_posterior_test, eval_viterbi_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_smothing = hmm.pick_best_smoothing(train_seq, dev_seq, [10, 1, 0.1, 0])\n",
    "\n",
    "hmm.train_supervised(train_seq, smoothing=best_smothing)\n",
    "viterbi_pred_test = hmm.viterbi_decode_corpus(test_seq)\n",
    "posterior_pred_test = hmm.posterior_decode_corpus(test_seq)\n",
    "eval_viterbi_test = hmm.evaluate_corpus(test_seq, viterbi_pred_test)\n",
    "eval_posterior_test = hmm.evaluate_corpus(test_seq, posterior_pred_test)\n",
    "print(\"Best Smoothing %f --  Test Set Accuracy: Posterior Decode %.3f, Viterbi Decode: %.3f\" % (best_smothing, eval_posterior_test, eval_viterbi_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = cm.build_confusion_matrix(test_seq.seq_list, viterbi_pred_test,\n",
    "                                             len(corpus.tag_dict), hmm.get_num_states())\n",
    "\n",
    "cm.plot_confusion_bar_graph(confusion_matrix, corpus.tag_dict,\n",
    "                            range(hmm.get_num_states()), 'Confusion matrix')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 2.10\n",
    "Implement the method to update the counts given the state and transition posteriors\n",
    "\n",
    "```\n",
    "def update_counts(self, sequence, state_posteriors, transition_posteriors):\n",
    "```\n",
    "\n",
    "Look at the code for EM algorithm in file ```sequences/hmm.py``` and check it for yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "````\n",
    "def update_counts(self, sequence, state_posteriors, transition_posteriors):\n",
    "    \"\"\" Used in the E-step in EM.\"\"\"\n",
    "\n",
    "    num_states = self.get_num_states()  # Number of states.\n",
    "    length = len(sequence.x)  # Length of the sequence.\n",
    "\n",
    "    # Take care of initial probs\n",
    "    for y in range(num_states):\n",
    "        self.initial_counts[y] += state_posteriors[0, y]\n",
    "    for pos in range(length):\n",
    "        x = sequence.x[pos]\n",
    "        for y in range(num_states):\n",
    "            self.emission_counts[x, y] += state_posteriors[pos, y]\n",
    "            if pos > 0:\n",
    "                for y_prev in range(num_states):\n",
    "                    self.transition_counts[y, y_prev] += transition_posteriors[pos-1, y, y_prev]\n",
    "\n",
    "    # Final position\n",
    "    for y in range(num_states):\n",
    "        self.final_counts[y] += state_posteriors[length-1, y]\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.11\n",
    "Run 20 epochs of the EM algorithm for part of speech induction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with EM.\n",
    "hmm.train_EM(train_seq, 0.1, 20, evaluate=True)\n",
    "viterbi_pred_test = hmm.viterbi_decode_corpus(test_seq)\n",
    "posterior_pred_test = hmm.posterior_decode_corpus(test_seq)\n",
    "eval_viterbi_test = hmm.evaluate_corpus(test_seq, viterbi_pred_test)\n",
    "eval_posterior_test = hmm.evaluate_corpus(test_seq, posterior_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = cm.build_confusion_matrix(test_seq.seq_list, viterbi_pred_test,\n",
    "                                             len(corpus.tag_dict), hmm.get_num_states())\n",
    "cm.plot_confusion_bar_graph(confusion_matrix, corpus.tag_dict,\n",
    "                            range(hmm.get_num_states()), 'Confusion matrix')"
   ]
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('lxmls-labs-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "b440f778a306812259839a053d6e31b51ac7f182425975b23ce9d8c376038d8f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
